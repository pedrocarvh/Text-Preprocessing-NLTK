{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK Text Analyzer: Processamento e Métricas de Vocabulário\n",
        "\n",
        "Este notebook realiza uma análise de texto em português utilizando a biblioteca **NLTK**, aplicando etapas de:\n",
        "- Tokenização  \n",
        "- Remoção de *stopwords*  \n",
        "- Extração de radicais (*stemming*)  \n",
        "- Cálculo de riqueza lexical  \n",
        "- Frequência dos principais radicais\n",
        "\n",
        "O texto usado como exemplo é um ensaio de opinião sobre a moral e política no Brasil.\n"
      ],
      "metadata": {
        "id": "kzkmOEt6puUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Instalação e Importação das Bibliotecas\n",
        "Nesta etapa, são instaladas e importadas as dependências necessárias para o processamento de texto.\n"
      ],
      "metadata": {
        "id": "lpfcHlnGwUwO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTKv1fZ1lykI",
        "outputId": "ffa8415c-2b0c-4ef1-91d4-6b0f04f351fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# Instalar a biblioteca NLTK\n",
        "!pip install nltk\n",
        "\n",
        "# Importar módulos necessários\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    nltk.data.find('stemmers/rslp')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt_tab')\n",
        "    nltk.download(\"stopwords\")\n",
        "    nltk.download(\"rslp\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Função de Pré-Processamento\n",
        "\n",
        "A função `pre_processar_texto()` executa:\n",
        "1. **Tokenização** – separa o texto em palavras.  \n",
        "2. **Remoção de *stopwords*** – elimina palavras muito comuns do português.  \n",
        "3. **Stemming** – reduz palavras a seus radicais.  \n",
        "\n",
        "Ela retorna duas listas:  \n",
        "- Lista de **radicais** (*stems*)  \n",
        "- Lista de **palavras originais filtradas**\n"
      ],
      "metadata": {
        "id": "wZhq1S8NxCTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processar_texto(texto):\n",
        "    \"\"\"\n",
        "    Executa Tokenização, Remoção de Stop Words e Stemming no texto.\n",
        "\n",
        "    Args:\n",
        "        texto (str): O texto a ser processado.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de radicais (stems) das palavras significativas.\n",
        "        list: Lista de palavras (tokens) antes do stemming.\n",
        "    \"\"\"\n",
        "    # 1. Configurar\n",
        "    stemmer = RSLPStemmer()\n",
        "    stop_words_pt = set(stopwords.words('portuguese'))\n",
        "\n",
        "    # 2. Tokenização e Normalização\n",
        "    # Encontra apenas palavras (letras), e converte para minúsculas\n",
        "    # Pontuação não foi selecionado.\n",
        "    palavras = re.findall(r'\\b[A-Za-zÀ-ú]+\\b', texto.lower())\n",
        "\n",
        "    # 3. Remoção de Stop Words\n",
        "    palavras_filtradas = [\n",
        "        palavra for palavra in palavras if palavra not in stop_words_pt\n",
        "    ]\n",
        "\n",
        "    # 4. Stemming\n",
        "    radicais = [\n",
        "        stemmer.stem(palavra) for palavra in palavras_filtradas\n",
        "    ]\n",
        "\n",
        "    return radicais, palavras\n"
      ],
      "metadata": {
        "id": "c6Njw3iYmwVx"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Texto de Entrada\n",
        "\n",
        "O texto abaixo serve como base para análise (o texto foi retirado de uma publicação no MEDIUM).\n",
        "\n",
        "https://medium.com/@ligandooexploda-se/um-triagulo-das-bermudas-chamado-brasil-4177d1c9a3ae\n",
        "\n",
        "Ele trata de reflexões políticas e sociais, permitindo explorar a diversidade e frequência vocabular.\n"
      ],
      "metadata": {
        "id": "YJ9CPSXXxJJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrada de Dados\n",
        "TEXTO_DE_ANALISE = \"\"\"\n",
        "Existe um triangulo em alto mar formato, pelas ilhas Bermudas ao norte, Porto Rico ao sul e a costa da Florida a oeste onde vez por outra noticiam-se estranhos sumiços de barcos e aeronaves. Especialistas estimam que o tamanho da região que compõem o Triangulo das Bermudas é de cerca de 1 milhão de km2. O Brasil tem 8.515.767,049 quilômetros quadrados, ou seja, temos mais de sete vezes o chamando do Triângulo das Bermudas onde objetos materiais como navios e aviões desaparecem.\n",
        "Imagino que dado seu tamanho, no Brasil desaparecem elementos materiais e digamos que outros nem tanto, como respeito, dignidade, princípios, caráter, honra e outros atributos que dignificam o “homem” na qualidade de figura humana. O Brasil está se tornando um autêntico deserto de qualidades morais.\n",
        "Se olharmos para outras Nações que sustentam seus projetos de direito, de live expressão e democracia, todas se baseiam na Justiça, já a nossa justiça tem estado sob dúvidas. Quando a imprensa divulga que esposas, irmãos, filhos e filhas de ministros do Superior Tribunal Federal, a instancia maior da nossa justiça, mitigam ações justamente naquela casa, e ninguém acha estranho ou imoral, constatamos que vergonha, respeito e dignidade são artigos raros no Brasil e seguimos em frente.\n",
        "Se olharmos para o nosso Congresso, vemos semanalmente de terça a quinta um escarnio atrás do outro, como verdadeiro tapa na cara da Sociedade pagadora de impostos. Como, por exemplo as emendas parlamentares, sem destino, sem autor e sem vergonha. Tão sem vergonha e certos da impunidade, que se permitiram aumentar o fundo eleitoral de R$ 2.034 bilhões em 2022 para R$ 4.960 bilhões para 2.026.\n",
        "Deve ser um dos poucos países do mundo em que a classe política cobra do cidadão para se eleger e usufruir de todas as benesses de ter um mandato. O que lhes dá o direito de ter imunidade parlamentar, que na minha opinião, é uma espécie de licença para traficar. Como todos os possuidores de imunidade parlamentar ao serem flagrados em suas traficâncias vão ser julgados pelo STF, e lá os processos dormem, de acordo com os interesses políticos do momento.\n",
        "Por fim, mas não menos importante, temos um governo cujo líder foi retirado da prisão exatamente por decisão do STF que o auxiliou para ganhar a eleição e, a partir daí, deixa-lo livre para pôr em execução um plano que ainda não está bem claro que rumo vai tomar. É obvio que o custo para satisfazer todos os envolvidos, sejam dos partícipes do judiciário, os do Congresso e os do governo transformaram o Ministro Fernando Haddad em um arrecadador de impostos insaciável, mesmo considerando a já brutal carga tributaria de pagamos.\n",
        "Como a maior preocupação deste governo é arrecadar, questões como INSS, metanol em bebidas alcoólicas , organizações criminosas infiltradas em todas areas da administração pública, são deixadas de lado até que a população esqueça, atropelada por outro escandalo. Mas há sinais claros para todos, mesmo os minimamente informados, que caminhamos para uma espécie de sistema de proletariado onde o governo decide tudo, paga tudo, transformando boa parte da Sociedade em dependentes do Estado e com isso ganha eleições duvidosas como em Cuba, Venezuela e, talvez em breve, no Brasil.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"--- Texto a ser Analisado (Amostra) ---\\n{TEXTO_DE_ANALISE.strip()[:150]}...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0kpZsvoxPWz",
        "outputId": "dd1ec006-2d63-49ac-b055-5a8b0e372952"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Texto a ser Analisado (Amostra) ---\n",
            "Existe um triangulo em alto mar formato, pelas ilhas Bermudas ao norte, Porto Rico ao sul e a costa da Florida a oeste onde vez por outra noticiam-se ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Execução do Processamento\n",
        "\n",
        "Nesta etapa, o texto é processado e são extraídos:\n",
        "- Palavras significativas\n",
        "- Radicais correspondentes  \n"
      ],
      "metadata": {
        "id": "bGzqMp6MxUQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execução do Processamento\n",
        "radicais, palavras_originais = pre_processar_texto(TEXTO_DE_ANALISE)\n",
        "\n",
        "palavras_filtradas = [\n",
        "    palavra for palavra in palavras_originais if palavra not in stop_words_pt\n",
        "    ]"
      ],
      "metadata": {
        "id": "u8C6RkJYxSz_"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Cálculo de Métricas\n",
        "\n",
        "Aqui são calculadas as métricas fundamentais:\n",
        "- **Riqueza Lexical** = (Palavras Únicas / Palavras Significativas Totais)\n",
        "- **Top 10 Radicais** mais frequentes\n"
      ],
      "metadata": {
        "id": "qCxP6TfPxam2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cálculo de Métricas ---\n",
        "\n",
        "# Riqueza Lexical\n",
        "total_palavras_significativas = len(palavras_filtradas)\n",
        "total_palavras_unicas = len(set(palavras_filtradas))\n",
        "total_radicais_unicos = len(set(radicais))\n",
        "\n",
        "riqueza_lexical = total_palavras_unicas / total_palavras_significativas if total_palavras_significativas > 0 else 0.0\n",
        "\n",
        "# Top 10 Radicais Mais Frequentes\n",
        "dist_frequencia = nltk.FreqDist(radicais)\n",
        "top_10_radicais = dist_frequencia.most_common(10)\n"
      ],
      "metadata": {
        "id": "BogQwnTExcKF"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Relatório Final\n",
        "\n",
        "O relatório exibe:\n",
        "- Estatísticas de vocabulário  \n",
        "- Diversidade lexical  \n",
        "- Lista dos radicais mais frequentes  \n"
      ],
      "metadata": {
        "id": "_15GTkDqxeVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cálculo de Métricas ---\n",
        "\n",
        "# Riqueza Lexical\n",
        "total_palavras_significativas = len(palavras_filtradas)\n",
        "total_palavras_unicas = len(set(palavras_filtradas))\n",
        "total_radicais_unicos = len(set(radicais))\n",
        "\n",
        "riqueza_lexical = total_palavras_unicas / total_palavras_significativas if total_palavras_significativas > 0 else 0.0\n",
        "\n",
        "# Top 10 Radicais Mais Frequentes\n",
        "dist_frequencia = nltk.FreqDist(radicais)\n",
        "top_10_radicais = dist_frequencia.most_common(10)\n",
        "\n",
        "\n",
        "# --- Relatório Final (Saída) ---\n",
        "\n",
        "print(\"=\"*40)\n",
        "print(\"     RELATÓRIO DE ANÁLISE DE TEXTO     \")\n",
        "print(\"=\"*40)\n",
        "\n",
        "print(\"\\n[1] Visão Geral do Vocabulário:\")\n",
        "print(f\" - Palavras Totais (Tokens): {len(palavras_originais)}\")\n",
        "print(f\" - Palavras Únicas: {total_palavras_unicas}\")\n",
        "print(f\" - Radicais Únicos: {total_radicais_unicos}\")\n",
        "print(f\" - Riqueza Lexical (Diversidade): **{riqueza_lexical:.4f}**\")\n",
        "print(\"   *Métrica: Palavras Únicas / Palavras Significativas Totais\")\n",
        "\n",
        "print(\"\\n[2] Frequência Temática (Top 10 Radicais):\")\n",
        "for i, (radical, contagem) in enumerate(top_10_radicais):\n",
        "    print(f\" {i+1}. '{radical}': {contagem} ocorrências\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEIBdz0Uq1uv",
        "outputId": "2b34cdf1-6e78-4c88-972e-4c0983d39787"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "     RELATÓRIO DE ANÁLISE DE TEXTO     \n",
            "========================================\n",
            "\n",
            "[1] Visão Geral do Vocabulário:\n",
            " - Palavras Totais (Tokens): 525\n",
            " - Palavras Únicas: 253\n",
            " - Radicais Únicos: 232\n",
            " - Riqueza Lexical (Diversidade): **0.8576**\n",
            "   *Métrica: Palavras Únicas / Palavras Significativas Totais\n",
            "\n",
            "[2] Frequência Temática (Top 10 Radicais):\n",
            " 1. 'outr': 6 ocorrências\n",
            " 2. 'tod': 6 ocorrências\n",
            " 3. 'brasil': 5 ocorrências\n",
            " 4. 'govern': 4 ocorrências\n",
            " 5. 'bermud': 3 ocorrências\n",
            " 6. 'ond': 3 ocorrências\n",
            " 7. 'justiç': 3 ocorrências\n",
            " 8. 'vergonh': 3 ocorrências\n",
            " 9. 'pag': 3 ocorrências\n",
            " 10. 'parlament': 3 ocorrências\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Considerações Finais\n",
        "\n",
        "O notebook demonstra de forma prática como aplicar técnicas de **pré-processamento textual** em português usando o **NLTK**.  \n",
        "A análise de riqueza lexical e frequência de radicais pode ser usada para:\n",
        "- Avaliar a complexidade de textos\n",
        "- Apoiar análise de sentimento e tópicos\n",
        "- Enriquecer pipelines de PLN em projetos reais\n"
      ],
      "metadata": {
        "id": "z9svDHkVxnWR"
      }
    }
  ]
}